<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="FlinkSQL(Flink1.15/1.16)源码分析" /><meta property="og:locale" content="en" /><meta name="description" content="FlinkSQL流程图 StreamSQLExample 示例代码 ``` def main(args: Array[String]): Unit = {" /><meta property="og:description" content="FlinkSQL流程图 StreamSQLExample 示例代码 ``` def main(args: Array[String]): Unit = {" /><link rel="canonical" href="/posts/FLINKSQL/" /><meta property="og:url" content="/posts/FLINKSQL/" /><meta property="og:site_name" content="2pc" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-08-30T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="FlinkSQL(Flink1.15/1.16)源码分析" /><meta name="twitter:site" content="@2pc" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-10-17T12:13:57+00:00","datePublished":"2022-08-30T00:00:00+00:00","description":"FlinkSQL流程图 StreamSQLExample 示例代码 ``` def main(args: Array[String]): Unit = {","headline":"FlinkSQL(Flink1.15/1.16)源码分析","mainEntityOfPage":{"@type":"WebPage","@id":"/posts/FLINKSQL/"},"url":"/posts/FLINKSQL/"}</script><title>FlinkSQL(Flink1.15/1.16)源码分析 | 2pc</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="2pc"><meta name="application-name" content="2pc"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">2pc</a></div><div class="site-subtitle font-italic">A text-focused Jekyll theme</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/2pc" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/2pc" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['2pcgithub','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>FlinkSQL(Flink1.15/1.16)源码分析</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>FlinkSQL(Flink1.15/1.16)源码分析</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1661817600" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Aug 30, 2022 </em> </span> <span> Updated <em class="" data-ts="1666008837" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Oct 17, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://twitter.com/2pc">2pc</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3780 words"> <em>21 min</em> read</span></div></div></div><div class="post-content"><h3 id="flinksql流程图"><span class="mr-2">FlinkSQL流程图</span><a href="#flinksql流程图" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="https://raw.githubusercontent.com/2pc/mydrawio/master/flink/export/FLinkSQL-%E7%AC%AC-1-%E9%A1%B5.png" alt="FlinkSQL" data-proofer-ignore> StreamSQLExample 示例代码</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre><td class="rouge-code"><pre>  def main(args: Array[String]): Unit = {

    // set up the Scala DataStream API
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    // set up the Scala Table API
    val tableEnv = StreamTableEnvironment.create(env)

    val orderA =
      env.fromCollection(Seq(Order(1L, "beer", 3), Order(1L, "diaper", 4), Order(3L, "rubber", 2)))

    val orderB =
      env.fromCollection(Seq(Order(2L, "pen", 3), Order(2L, "rubber", 3), Order(4L, "beer", 1)))

    // convert the first DataStream to a Table object
    // it will be used "inline" and is not registered in a catalog
    val tableA = tableEnv.fromDataStream(orderA)

    // convert the second DataStream and register it as a view
    // it will be accessible under a name
    tableEnv.createTemporaryView("TableB", orderB)

    // union the two tables
    val result = tableEnv.sqlQuery(s"""
                                      |SELECT * FROM $tableA WHERE amount &gt; 2
                                      |UNION ALL
                                      |SELECT * FROM TableB WHERE amount &lt; 2
        """.stripMargin)

    // convert the Table back to an insert-only DataStream of type `Order`
    tableEnv.toDataStream(result, classOf[Order]).print()

    // after the table program is converted to a DataStream program,
    // we must use `env.execute()` to submit the job
    env.execute()
  }
</pre></table></code></div></div><p>StreamTableEnvironment.sqlQuery</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre>    public Table sqlQuery(String query) {
        //将SQL解析成SqlNode，生成逻辑执行计划，并封装成Operation，普通select语句就是，PlannerQueryOperation
        List&lt;Operation&gt; operations = getParser().parse(query);

        if (operations.size() != 1) {
            throw new ValidationException(
                    "Unsupported SQL query! sqlQuery() only accepts a single SQL query.");
        }

        Operation operation = operations.get(0);

        if (operation instanceof QueryOperation &amp;&amp; !(operation instanceof ModifyOperation)) {
            return createTable((QueryOperation) operation);
        } else {
            throw new ValidationException(
                    "Unsupported SQL query! sqlQuery() only accepts a single SQL query of type "
                            + "SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.");
        }
    }
</pre></table></code></div></div><ol><li>SQL解析阶段</ol><p>生成AST（抽象语法树），将SQL转化为SqlNode</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre>public List&lt;Operation&gt; parse(String statement) {
    CalciteParser parser = calciteParserSupplier.get();
    FlinkPlannerImpl planner = validatorSupplier.get();

    Optional&lt;Operation&gt; command = EXTENDED_PARSER.parse(statement);
    if (command.isPresent()) {
        return Collections.singletonList(command.get());
    }

    // parse the sql query
    // use parseSqlList here because we need to support statement end with ';' in sql client.
    //解析sql生成SqlNode,
    SqlNodeList sqlNodeList = parser.parseSqlList(statement);
    List&lt;SqlNode&gt; parsed = sqlNodeList.getList();
    Preconditions.checkArgument(parsed.size() == 1, "only single statement supported");
    return Collections.singletonList(
        SqlToOperationConverter.convert(planner, catalogManager, parsed.get(0))
        .orElseThrow(() -&gt; new TableException("Unsupported query: " + statement)));
}
</pre></table></code></div></div><ol><li>验证阶段，SqlNode –&gt;SqlNode<br /> ``` public static Optional<Operation> convert( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode sqlNode) { // validate the query //验证阶段,还是SqlNode final SqlNode validated = flinkPlanner.validate(sqlNode); return convertValidatedSqlNode(flinkPlanner, catalogManager, validated); }</Operation></ol><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>3. 语义分析,生成逻辑计划树（SqlNode–&gt;RelNode/RexNode）   
</pre></table></code></div></div><p>// /** Convert a validated sql node to Operation. */ private static Optional<Operation> convertValidatedSqlNode( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode validated) { //这里是Kind==UNION if (validated.getKind().belongsTo(SqlKind.QUERY)) { return Optional.of(converter.convertSqlQuery(validated)); }</Operation></p><p>} //SqlToOperationConverter private Operation convertSqlQuery(SqlNode node) { return toQueryOperation(flinkPlanner, node); } //SqlToOperationConverter //生成relNode,并封装成PlannerQueryOperation private PlannerQueryOperation toQueryOperation(FlinkPlannerImpl planner, SqlNode validated) { // transform to a relational tree RelRoot relational = planner.rel(validated); return new PlannerQueryOperation(relational.project()); }</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>
planner的实现类是FlinkPlannerImpl   
</pre></table></code></div></div><p>//FlinkPlannerImpl def rel(validatedSqlNode: SqlNode): RelRoot = { rel(validatedSqlNode, getOrCreateSqlValidator())//FlinkCalciteSqlValidator } //FlinkPlannerImpl private def rel(validatedSqlNode: SqlNode, sqlValidator: FlinkCalciteSqlValidator) = { try { assert(validatedSqlNode != null) val sqlToRelConverter: SqlToRelConverter = createSqlToRelConverter(sqlValidator)</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
</pre><td class="rouge-code"><pre>    sqlToRelConverter.convertQuery(validatedSqlNode, false, true)
    // we disable automatic flattening in order to let composite types pass without modification
    // we might enable it again once Calcite has better support for structured types
    // root = root.withRel(sqlToRelConverter.flattenTypes(root.rel, true))

    // TableEnvironment.optimize will execute the following
    // root = root.withRel(RelDecorrelator.decorrelateQuery(root.rel))
    // convert time indicators
    // root = root.withRel(RelTimeIndicatorConverter.convert(root.rel, rexBuilder))
} catch {
    case e: RelConversionException =&gt; throw new TableException(e.getMessage)
    } } ``` 定义FlinkCalciteSqlValidator，SqlToRelConverter就是是Calcite的&lt;br /&gt;SqlToRelConverter.convertQueryRecursive将SqlNode转RelNode    ``` //SqlToRelConverter public RelRoot convertQuery(SqlNode query, boolean needsValidation, boolean top) {
if (needsValidation) {
    query = this.validator.validate(query);
}

RelNode result = this.convertQueryRecursive(query, top, (RelDataType)null).rel;
if (top &amp;&amp; isStream(query)) {
    result = new LogicalDelta(this.cluster, ((RelNode)result).getTraitSet(), (RelNode)result);
}

RelCollation collation = RelCollations.EMPTY;
if (!query.isA(SqlKind.DML) &amp;&amp; isOrdered(query)) {
    collation = this.requiredCollation((RelNode)result);
}

this.checkConvertedType(query, (RelNode)result);
if (SQL2REL_LOGGER.isDebugEnabled()) {
    SQL2REL_LOGGER.debug(RelOptUtil.dumpPlan("Plan after converting SqlNode to RelNode", (RelNode)result, SqlExplainFormat.TEXT, SqlExplainLevel.EXPPLAN_ATTRIBUTES));
}

RelDataType validatedRowType = this.validator.getValidatedNodeType(query);
List&lt;RelHint&gt; hints = new ArrayList();
if (query.getKind() == SqlKind.SELECT) {
    SqlSelect select = (SqlSelect)query;
    if (select.hasHints()) {
        hints = SqlUtil.getRelHint(this.hintStrategies, select.getHints());
    }
}

RelNode result = RelOptUtil.propagateRelHints((RelNode)result, false);
return RelRoot.of(result, validatedRowType, query.getKind()).withCollation(collation).withHints((List)hints); } ```
</pre></table></code></div></div><ol><li>优化阶段（RelNode–&gt;RelNode）</ol><p>tableEnv.toDataStream(result, Order.class) <br />toStreamInternal–&gt;planner.translate</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre><td class="rouge-code"><pre>public &lt;T&gt; DataStream&lt;T&gt; toDataStream(Table table, Class&lt;T&gt; targetClass) {
    Preconditions.checkNotNull(table, "Table must not be null.");
    Preconditions.checkNotNull(targetClass, "Target class must not be null.");
    if (targetClass == Row.class) {//targetClass==Order.class
        // for convenience, we allow the Row class here as well
        return (DataStream&lt;T&gt;) toDataStream(table);//这里
    }

    return toDataStream(table, DataTypes.of(targetClass));
}
//
public &lt;T&gt; DataStream&lt;T&gt; toDataStream(Table table, AbstractDataType&lt;?&gt; targetDataType) {
    Preconditions.checkNotNull(table, "Table must not be null.");
    Preconditions.checkNotNull(targetDataType, "Target data type must not be null.");

    final SchemaTranslator.ProducingResult schemaTranslationResult =
    SchemaTranslator.createProducingResult(
        getCatalogManager().getDataTypeFactory(),
        table.getResolvedSchema(),
        targetDataType);

    return toStreamInternal(table, schemaTranslationResult, ChangelogMode.insertOnly());
}
//
protected &lt;T&gt; DataStream&lt;T&gt; toStreamInternal(
    Table table,
    SchemaTranslator.ProducingResult schemaTranslationResult,
    @Nullable ChangelogMode changelogMode) {
    //省略其他代码,只看toStreamInternal
    return toStreamInternal(table, modifyOperation);
}
//
protected &lt;T&gt; DataStream&lt;T&gt; toStreamInternal(Table table, ModifyOperation modifyOperation) {
    //PlannerBase.translate
    final List&lt;Transformation&lt;?&gt;&gt; transformations =
    planner.translate(Collections.singletonList(modifyOperation));

    final Transformation&lt;T&gt; transformation = getTransformation(table, transformations);
    executionEnvironment.addOperator(transformation);

    // Reconfigure whenever planner transformations are added
    // We pass only the configuration to avoid reconfiguration with the rootConfiguration
    executionEnvironment.configure(tableConfig.getConfiguration());

    return new DataStream&lt;&gt;(executionEnvironment, transformation);
}
</pre></table></code></div></div><p>PlannerBase</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre>override def translate(
    modifyOperations: util.List[ModifyOperation]): util.List[Transformation[_]] = {
    beforeTranslation()
    if (modifyOperations.isEmpty) {
        return List.empty[Transformation[_]]
    }
    //这里是modifyOperations 有ExternalModifyOperation等,生成一个DataStreamTableSink
    val relNodes = modifyOperations.map(translateToRel)
    //1.优化RelNode到FlinkLogicalRel,
    //2. 物理规则阶段优化FlinkLogicalRel到FlinkPhysicalRel
    val optimizedRelNodes = optimize(relNodes)
    //生成execGraph，即FlinkPhysicalRel转换成execGraph
    val execGraph = translateToExecNodeGraph(optimizedRelNodes)
    //生成物理执行计划,这里实现类StreamPlanner和BatchPlanner
    val transformations = translateToPlan(execGraph)
    afterTranslation()
    transformations
}
</pre></table></code></div></div><p>PlannerBase逻辑执行计划优化<br />1.15版本</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre>private[flink] def optimize(relNodes: Seq[RelNode]): Seq[RelNode] = {
    val optimizedRelNodes = getOptimizer.optimize(relNodes)//
    require(optimizedRelNodes.size == relNodes.size)
    optimizedRelNodes
}
//CommonSubGraphBasedOptimizer
override def optimize(roots: Seq[RelNode]): Seq[RelNode] = {
    //
    val sinkBlocks = doOptimize(roots)
    val optimizedPlan = sinkBlocks.map {
        block =&gt;
        val plan = block.getOptimizedPlan
        require(plan != null)
        plan
    }
    expandIntermediateTableScan(optimizedPlan)
}
</pre></table></code></div></div><p>1.16版本</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre><td class="rouge-code"><pre>  override def optimize(roots: Seq[RelNode]): Seq[RelNode] = {
    // resolve hints before optimizing
    val joinHintResolver = new JoinHintResolver()
    val resolvedHintRoots = joinHintResolver.resolve(toJava(roots))

    // clear query block alias bef optimizing
    val clearQueryBlockAliasResolver = new ClearQueryBlockAliasResolver
    val resolvedAliasRoots = clearQueryBlockAliasResolver.resolve(resolvedHintRoots)
    //doOptimize 两种实现流批
    val sinkBlocks = doOptimize(resolvedAliasRoots)
    val optimizedPlan = sinkBlocks.map {
      block =&gt;
        val plan = block.getOptimizedPlan
        require(plan != null)
        plan
    }
    val expanded = expandIntermediateTableScan(optimizedPlan)

    val postOptimizedPlan = postOptimize(expanded)

    // Rewrite same rel object to different rel objects
    // in order to get the correct dag (dag reuse is based on object not digest)
    val shuttle = new SameRelObjectShuttle()
    val relsWithoutSameObj = postOptimizedPlan.map(_.accept(shuttle))

    // reuse subplan
    SubplanReuser.reuseDuplicatedSubplan(relsWithoutSameObj, unwrapTableConfig(roots.head))
  }
</pre></table></code></div></div><p>doOptimize由子类实现，流批分开：StreamCommonSubGraphBasedOptimizer，BatchCommonSubGraphBasedOptimizer<br />主要两个方法 optimizeTree ，optimizeBlock<br />StreamCommonSubGraphBasedOptimizer.doOptimize</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
</pre><td class="rouge-code"><pre>override protected def doOptimize(roots: Seq[RelNode]): Seq[RelNodeBlock] = {
val tableConfig = planner.getTableConfig
// build RelNodeBlock plan
val sinkBlocks = RelNodeBlockPlanBuilder.buildRelNodeBlockPlan(roots, tableConfig)
// infer trait properties for sink block
sinkBlocks.foreach {
  sinkBlock =&gt;
    // don't require update before by default
    sinkBlock.setUpdateBeforeRequired(false)

    val miniBatchInterval: MiniBatchInterval =
      if (tableConfig.get(ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_ENABLED)) {
        val miniBatchLatency =
          tableConfig.get(ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_ALLOW_LATENCY).toMillis
        Preconditions.checkArgument(
          miniBatchLatency &gt; 0,
          "MiniBatch Latency must be greater than 0 ms.",
          null)
        new MiniBatchInterval(miniBatchLatency, MiniBatchMode.ProcTime)
      } else {
        MiniBatchIntervalTrait.NONE.getMiniBatchInterval
      }
    sinkBlock.setMiniBatchInterval(miniBatchInterval)
}

if (sinkBlocks.size == 1) {
  // If there is only one sink block, the given relational expressions are a simple tree
  // (only one root), not a dag. So many operations (e.g. infer and propagate
  // requireUpdateBefore) can be omitted to save optimization time.
  val block = sinkBlocks.head
  val optimizedTree = optimizeTree(
    block.getPlan,
    block.isUpdateBeforeRequired,
    block.getMiniBatchInterval,
    isSinkBlock = true)
  block.setOptimizedPlan(optimizedTree)
  return sinkBlocks
}
//optimizeBlock  
// TODO FLINK-24048: Move changeLog inference out of optimizing phase
// infer modifyKind property for each blocks independently
sinkBlocks.foreach(b =&gt; optimizeBlock(b, isSinkBlock = true))
// infer and propagate updateKind and miniBatchInterval property for each blocks
sinkBlocks.foreach {
  b =&gt;
    propagateUpdateKindAndMiniBatchInterval(
      b,
      b.isUpdateBeforeRequired,
      b.getMiniBatchInterval,
      isSinkBlock = true)
}
// clear the intermediate result
sinkBlocks.foreach(resetIntermediateResult)
// optimize recursively RelNodeBlock
sinkBlocks.foreach(b =&gt; optimizeBlock(b, isSinkBlock = true))
sinkBlocks
}
</pre></table></code></div></div><p>BatchCommonSubGraphBasedOptimizer.doOptimize</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre>override protected def doOptimize(roots: Seq[RelNode]): Seq[RelNodeBlock] = {
// build RelNodeBlock plan
val rootBlocks =
  RelNodeBlockPlanBuilder.buildRelNodeBlockPlan(roots, planner.getTableConfig)
// optimize recursively RelNodeBlock
rootBlocks.foreach(optimizeBlock)
rootBlocks
}

private def optimizeBlock(block: RelNodeBlock): Unit = {
block.children.foreach {
  child =&gt;
    if (child.getNewOutputNode.isEmpty) {
      optimizeBlock(child)
    }
}

val originTree = block.getPlan
val optimizedTree = optimizeTree(originTree)

optimizedTree match {
  case _: LegacySink | _: Sink =&gt; // ignore
  case _ =&gt;
    val name = createUniqueIntermediateRelTableName
    val intermediateRelTable =
      new IntermediateRelTable(Collections.singletonList(name), optimizedTree)
    val newTableScan = wrapIntermediateRelTableToTableScan(intermediateRelTable, name)
    block.setNewOutputNode(newTableScan)
    block.setOutputTableName(name)
}
block.setOptimizedPlan(optimizedTree)
}
</pre></table></code></div></div><p>看下流的optimizeTree<br />StreamCommonSubGraphBasedOptimizer.optimizeTree</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
</pre><td class="rouge-code"><pre>private def optimizeTree(
  relNode: RelNode,
  updateBeforeRequired: Boolean,
  miniBatchInterval: MiniBatchInterval,
  isSinkBlock: Boolean): RelNode = {

val tableConfig = planner.getTableConfig
val calciteConfig = TableConfigUtils.getCalciteConfig(tableConfig)
//注意programs的构建FlinkStreamProgram.buildProgram
val programs = calciteConfig.getStreamProgram
  .getOrElse(FlinkStreamProgram.buildProgram(tableConfig))
Preconditions.checkNotNull(programs)

val context = unwrapContext(relNode)
//FlinkChainedProgram
programs.optimize(
  relNode,
  new StreamOptimizeContext() {

    override def isBatchMode: Boolean = false

    override def getTableConfig: TableConfig = tableConfig

    override def getFunctionCatalog: FunctionCatalog = planner.functionCatalog

    override def getCatalogManager: CatalogManager = planner.catalogManager

    override def getModuleManager: ModuleManager = planner.moduleManager

    override def getRexFactory: RexFactory = context.getRexFactory

    override def getFlinkRelBuilder: FlinkRelBuilder = planner.createRelBuilder

    override def isUpdateBeforeRequired: Boolean = updateBeforeRequired

    def getMiniBatchInterval: MiniBatchInterval = miniBatchInterval

    override def needFinalTimeIndicatorConversion: Boolean = isSinkBlock

    override def getClassLoader: ClassLoader = context.getClassLoader
  }
)
}

</pre></table></code></div></div><p>这里的programs是FlinkStreamProgram.buildProgram生成的</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
</pre><td class="rouge-code"><pre>def buildProgram(tableConfig: ReadableConfig): FlinkChainedProgram[StreamOptimizeContext] = {
val chainedProgram = new FlinkChainedProgram[StreamOptimizeContext]()

// rewrite sub-queries to joins
chainedProgram.addLast(
  SUBQUERY_REWRITE,
  FlinkGroupProgramBuilder
    .newBuilder[StreamOptimizeContext]
    // rewrite QueryOperationCatalogViewTable before rewriting sub-queries
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.TABLE_REF_RULES)
        .build(),
      "convert table references before rewriting sub-queries to semi-join"
    )
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.SEMI_JOIN_RULES)
        .build(),
      "rewrite sub-queries to semi-join"
    )
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.TABLE_SUBQUERY_RULES)
        .build(),
      "sub-queries remove"
    )
    // convert RelOptTableImpl (which exists in SubQuery before) to FlinkRelOptTable
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.TABLE_REF_RULES)
        .build(),
      "convert table references after sub-queries removed"
    )
    .build()
)

// rewrite special temporal join plan
chainedProgram.addLast(
  TEMPORAL_JOIN_REWRITE,
  FlinkGroupProgramBuilder
    .newBuilder[StreamOptimizeContext]
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.EXPAND_PLAN_RULES)
        .build(),
      "convert correlate to temporal table join"
    )
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.POST_EXPAND_CLEAN_UP_RULES)
        .build(),
      "convert enumerable table scan"
    )
    .build()
)

// query decorrelation
chainedProgram.addLast(
  DECORRELATE,
  FlinkGroupProgramBuilder
    .newBuilder[StreamOptimizeContext]
    // rewrite before decorrelation
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.PRE_DECORRELATION_RULES)
        .build(),
      "pre-rewrite before decorrelation"
    )
    .addProgram(new FlinkDecorrelateProgram)
    .build()
)

// default rewrite, includes: predicate simplification, expression reduction, window
// properties rewrite, etc.
chainedProgram.addLast(
  DEFAULT_REWRITE,
  FlinkHepRuleSetProgramBuilder.newBuilder
    .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
    .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
    .add(FlinkStreamRuleSets.DEFAULT_REWRITE_RULES)
    .build()
)

// rule based optimization: push down predicate(s) in where clause, so it only needs to read
// the required data
chainedProgram.addLast(
  PREDICATE_PUSHDOWN,
  FlinkGroupProgramBuilder
    .newBuilder[StreamOptimizeContext]
    .addProgram(
      FlinkGroupProgramBuilder
        .newBuilder[StreamOptimizeContext]
        .addProgram(
          FlinkHepRuleSetProgramBuilder
            .newBuilder[StreamOptimizeContext]
            .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
            .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
            .add(FlinkStreamRuleSets.JOIN_PREDICATE_REWRITE_RULES)
            .build(),
          "join predicate rewrite"
        )
        .addProgram(
          FlinkHepRuleSetProgramBuilder.newBuilder
            .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)
            .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
            .add(FlinkStreamRuleSets.FILTER_PREPARE_RULES)
            .build(),
          "filter rules"
        )
        .setIterations(5)
        .build(),
      "predicate rewrite"
    )
    .addProgram(
      // PUSH_PARTITION_DOWN_RULES should always be in front of PUSH_FILTER_DOWN_RULES
      // to prevent PUSH_FILTER_DOWN_RULES from consuming the predicates in partitions
      FlinkGroupProgramBuilder
        .newBuilder[StreamOptimizeContext]
        .addProgram(
          FlinkHepRuleSetProgramBuilder.newBuilder
            .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
            .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
            .add(FlinkStreamRuleSets.PUSH_PARTITION_DOWN_RULES)
            .build(),
          "push down partitions into table scan"
        )
        .addProgram(
          FlinkHepRuleSetProgramBuilder.newBuilder
            .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
            .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
            .add(FlinkStreamRuleSets.PUSH_FILTER_DOWN_RULES)
            .build(),
          "push down filters into table scan"
        )
        .build(),
      "push predicate into table scan"
    )
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.PRUNE_EMPTY_RULES)
        .build(),
      "prune empty after predicate push down"
    )
    .build()
)

// join reorder
if (tableConfig.get(OptimizerConfigOptions.TABLE_OPTIMIZER_JOIN_REORDER_ENABLED)) {
  chainedProgram.addLast(
    JOIN_REORDER,
    FlinkGroupProgramBuilder
      .newBuilder[StreamOptimizeContext]
      .addProgram(
        FlinkHepRuleSetProgramBuilder.newBuilder
          .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)
          .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
          .add(FlinkStreamRuleSets.JOIN_REORDER_PREPARE_RULES)
          .build(),
        "merge join into MultiJoin"
      )
      .addProgram(
        FlinkHepRuleSetProgramBuilder.newBuilder
          .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
          .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
          .add(FlinkStreamRuleSets.JOIN_REORDER_RULES)
          .build(),
        "do join reorder"
      )
      .build()
  )
}

// project rewrite
chainedProgram.addLast(
  PROJECT_REWRITE,
  FlinkHepRuleSetProgramBuilder.newBuilder
    .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)
    .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
    .add(FlinkStreamRuleSets.PROJECT_RULES)
    .build()
)

// optimize the logical plan
chainedProgram.addLast(
  LOGICAL,
  FlinkVolcanoProgramBuilder.newBuilder
    .add(FlinkStreamRuleSets.LOGICAL_OPT_RULES)
    .setRequiredOutputTraits(Array(FlinkConventions.LOGICAL))
    .build()
)

// logical rewrite
chainedProgram.addLast(
  LOGICAL_REWRITE,
  FlinkHepRuleSetProgramBuilder.newBuilder
    .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
    .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
    .add(FlinkStreamRuleSets.LOGICAL_REWRITE)
    .build()
)

// convert time indicators
chainedProgram.addLast(TIME_INDICATOR, new FlinkRelTimeIndicatorProgram)

// optimize the physical plan
chainedProgram.addLast(
  PHYSICAL,
  FlinkVolcanoProgramBuilder.newBuilder
    .add(FlinkStreamRuleSets.PHYSICAL_OPT_RULES)
    .setRequiredOutputTraits(Array(FlinkConventions.STREAM_PHYSICAL))
    .build()
)

// physical rewrite
chainedProgram.addLast(
  PHYSICAL_REWRITE,
  FlinkGroupProgramBuilder
    .newBuilder[StreamOptimizeContext]
    // add a HEP program for watermark transpose rules to make this optimization deterministic
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.WATERMARK_TRANSPOSE_RULES)
        .build(),
      "watermark transpose"
    )
    .addProgram(new FlinkChangelogModeInferenceProgram, "Changelog mode inference")
    .addProgram(
      new FlinkMiniBatchIntervalTraitInitProgram,
      "Initialization for mini-batch interval inference")
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.TOP_DOWN)
        .add(FlinkStreamRuleSets.MINI_BATCH_RULES)
        .build(),
      "mini-batch interval rules"
    )
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.PHYSICAL_REWRITE)
        .build(),
      "physical rewrite"
    )
    .build()
)

chainedProgram
}
</pre></table></code></div></div><p>可以看出来是一个链组成的，看定义大概有</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre>val SUBQUERY_REWRITE = "subquery_rewrite"
val TEMPORAL_JOIN_REWRITE = "temporal_join_rewrite"
val DECORRELATE = "decorrelate"
val DEFAULT_REWRITE = "default_rewrite"
val PREDICATE_PUSHDOWN = "predicate_pushdown"
val JOIN_REORDER = "join_reorder"
val PROJECT_REWRITE = "project_rewrite"
val LOGICAL = "logical"
val LOGICAL_REWRITE = "logical_rewrite"
val TIME_INDICATOR = "time_indicator"
val PHYSICAL = "physical"
val PHYSICAL_REWRITE = "physical_rewrite"
</pre></table></code></div></div><p>FlinkChainedProgram.optimize</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre>  def optimize(root: RelNode, context: OC): RelNode = {
    programNames.foldLeft(root) {
      (input, name) =&gt;
        val program = get(name).getOrElse(throw new TableException(s"This should not happen."))

        val start = System.currentTimeMillis()
        // program是FlinkOptimizeProgram
        //FlinkOptimizeProgram子类由FlinkHepProgram或者FlinkVolcanoProgram等
        val result = program.optimize(input, context)
        val end = System.currentTimeMillis()

        if (LOG.isDebugEnabled) {
          LOG.debug(
            s"optimize $name cost ${end - start} ms.\n" +
              s"optimize result: \n${FlinkRelOptUtil.toString(result)}")
        }

        result
    }
  }
</pre></table></code></div></div><p>programNames可以看出是所有的program列表遍历一遍，以PREDICATE_PUSHDOWN为例,其定义如下</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
</pre><td class="rouge-code"><pre>// rule based optimization: push down predicate(s) in where clause, so it only needs to read
// the required data
chainedProgram.addLast(
  PREDICATE_PUSHDOWN,
  FlinkGroupProgramBuilder
    .newBuilder[StreamOptimizeContext]
    .addProgram(
      FlinkGroupProgramBuilder
        .newBuilder[StreamOptimizeContext]
        .addProgram(
          FlinkHepRuleSetProgramBuilder
            .newBuilder[StreamOptimizeContext]
            .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
            .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
            .add(FlinkStreamRuleSets.JOIN_PREDICATE_REWRITE_RULES)
            .build(),
          "join predicate rewrite"
        )
        .addProgram(
          FlinkHepRuleSetProgramBuilder.newBuilder
            .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)
            .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
            .add(FlinkStreamRuleSets.FILTER_PREPARE_RULES)
            .build(),
          "filter rules"
        )
        .setIterations(5)
        .build(),
      "predicate rewrite"
    )
    .addProgram(
      // PUSH_PARTITION_DOWN_RULES should always be in front of PUSH_FILTER_DOWN_RULES
      // to prevent PUSH_FILTER_DOWN_RULES from consuming the predicates in partitions
      FlinkGroupProgramBuilder
        .newBuilder[StreamOptimizeContext]
        .addProgram(
          FlinkHepRuleSetProgramBuilder.newBuilder
            .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
            .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
            .add(FlinkStreamRuleSets.PUSH_PARTITION_DOWN_RULES)
            .build(),
          "push down partitions into table scan"
        )
        .addProgram(
          FlinkHepRuleSetProgramBuilder.newBuilder
            .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
            .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
            .add(FlinkStreamRuleSets.PUSH_FILTER_DOWN_RULES)
            .build(),
          "push down filters into table scan"
        )
        .build(),
      "push predicate into table scan"
    )
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.PRUNE_EMPTY_RULES)
        .build(),
      "prune empty after predicate push down"
    )
    .build()
)
</pre></table></code></div></div><p>这个也是多条，包括三个组”predicate rewrite”<br />FlinkStreamRuleSets.JOIN_PREDICATE_REWRITE_RULES<br />FlinkStreamRuleSets.FILTER_PREPARE_RULES</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre><td class="rouge-code"><pre>.addProgram(
  FlinkGroupProgramBuilder
    .newBuilder[StreamOptimizeContext]
    .addProgram(
      FlinkHepRuleSetProgramBuilder
        .newBuilder[StreamOptimizeContext]
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.JOIN_PREDICATE_REWRITE_RULES)
        .build(),
      "join predicate rewrite"
    )
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.FILTER_PREPARE_RULES)
        .build(),
      "filter rules"
    )
    .setIterations(5)
    .build(),
  "predicate rewrite"
)

</pre></table></code></div></div><p>push predicate into table scan<br />FlinkStreamRuleSets.PUSH_PARTITION_DOWN_RULES<br />FlinkStreamRuleSets.PUSH_FILTER_DOWN_RULES</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre><td class="rouge-code"><pre>.addProgram(
  // PUSH_PARTITION_DOWN_RULES should always be in front of PUSH_FILTER_DOWN_RULES
  // to prevent PUSH_FILTER_DOWN_RULES from consuming the predicates in partitions
  FlinkGroupProgramBuilder
    .newBuilder[StreamOptimizeContext]
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.PUSH_PARTITION_DOWN_RULES)
        .build(),
      "push down partitions into table scan"
    )
    .addProgram(
      FlinkHepRuleSetProgramBuilder.newBuilder
        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
        .add(FlinkStreamRuleSets.PUSH_FILTER_DOWN_RULES)
        .build(),
      "push down filters into table scan"
    )
    .build(),
  "push predicate into table scan"
)

</pre></table></code></div></div><p>prune empty after predicate push down<br />FlinkStreamRuleSets.PRUNE_EMPTY_RULES</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre>.addProgram(
  FlinkHepRuleSetProgramBuilder.newBuilder
    .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
    .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
    .add(FlinkStreamRuleSets.PRUNE_EMPTY_RULES)
    .build(),
  "prune empty after predicate push down"
)
.build()
</pre></table></code></div></div><p>逻辑执行计划<br />LOGICAL_CONVERTERS负责转换RelNode到FlinkLogicalRel转换</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre>/** RuleSet to do logical optimize for stream */
val LOGICAL_OPT_RULES: RuleSet = RuleSets.ofList(
(
  FILTER_RULES.asScala ++
    PROJECT_RULES.asScala ++
    PRUNE_EMPTY_RULES.asScala ++
    LOGICAL_RULES.asScala ++
    LOGICAL_CONVERTERS.asScala//转换RelNode --&gt;FlinkLogicalRel
).asJava)
</pre></table></code></div></div><p>LOGICAL_CONVERTERS</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre><td class="rouge-code"><pre>/** RuleSet to translate calcite nodes to flink nodes */
private val LOGICAL_CONVERTERS: RuleSet = RuleSets.ofList(
// translate to flink logical rel nodes
FlinkLogicalAggregate.STREAM_CONVERTER,
FlinkLogicalTableAggregate.CONVERTER,
FlinkLogicalOverAggregate.CONVERTER,
FlinkLogicalCalc.CONVERTER,
FlinkLogicalCorrelate.CONVERTER,
FlinkLogicalJoin.CONVERTER,
FlinkLogicalSort.STREAM_CONVERTER,
FlinkLogicalUnion.CONVERTER,
FlinkLogicalValues.CONVERTER,
FlinkLogicalTableSourceScan.CONVERTER,
FlinkLogicalLegacyTableSourceScan.CONVERTER,
FlinkLogicalTableFunctionScan.CONVERTER,
FlinkLogicalDataStreamTableScan.CONVERTER,
FlinkLogicalIntermediateTableScan.CONVERTER,
FlinkLogicalExpand.CONVERTER,
FlinkLogicalRank.CONVERTER,
FlinkLogicalWatermarkAssigner.CONVERTER,
FlinkLogicalWindowAggregate.CONVERTER,
FlinkLogicalWindowTableAggregate.CONVERTER,
FlinkLogicalSnapshot.CONVERTER,
FlinkLogicalMatch.CONVERTER,
FlinkLogicalSink.CONVERTER,
FlinkLogicalLegacySink.CONVERTER
)
</pre></table></code></div></div><p>比如FlinkLogicalJoinConverter负责join的转换<br />即join RelNode–&gt;FlinkLogicalJoin</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre><td class="rouge-code"><pre>/** Support all joins. */
private class FlinkLogicalJoinConverter
  extends ConverterRule(
    classOf[LogicalJoin],
    Convention.NONE,
    FlinkConventions.LOGICAL,
    "FlinkLogicalJoinConverter") {

  override def convert(rel: RelNode): RelNode = {
    val join = rel.asInstanceOf[LogicalJoin]
    val newLeft = RelOptRule.convert(join.getLeft, FlinkConventions.LOGICAL)
    val newRight = RelOptRule.convert(join.getRight, FlinkConventions.LOGICAL)
    FlinkLogicalJoin.create(newLeft, newRight, join.getCondition, join.getHints, join.getJoinType)
  }
}

object FlinkLogicalJoin {
  val CONVERTER: ConverterRule = new FlinkLogicalJoinConverter
//创建FlinkLogicalJoin
  def create(
      left: RelNode,
      right: RelNode,
      conditionExpr: RexNode,
      hints: JList[RelHint],
      joinType: JoinRelType): FlinkLogicalJoin = {
    val cluster = left.getCluster
    val traitSet = cluster.traitSetOf(FlinkConventions.LOGICAL).simplify()
    new FlinkLogicalJoin(cluster, traitSet, left, right, conditionExpr, hints, joinType)
  }
}
</pre></table></code></div></div><p>同理经过PHYSICAL，PHYSICAL_REWRITE优化转换后，得到FlinkPhysicalRel<br />比如StreamPhysicalDataStreamScanRule转换为StreamPhysicalDataStreamScan<br />StreamPhysicalTableSourceScanRule转换位StreamPhysicalChangelogNormalize<br />之后FlinkPhysicalRel转换为ExecNodeGraph</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre>  private[flink] def translateToExecNodeGraph(
      optimizedRelNodes: Seq[RelNode],
      isCompiled: Boolean): ExecNodeGraph = {
    val nonPhysicalRel = optimizedRelNodes.filterNot(_.isInstanceOf[FlinkPhysicalRel])
    if (nonPhysicalRel.nonEmpty) {
      throw new TableException(
        "The expected optimized plan is FlinkPhysicalRel plan, " +
          s"actual plan is ${nonPhysicalRel.head.getClass.getSimpleName} plan.")
    }

    require(optimizedRelNodes.forall(_.isInstanceOf[FlinkPhysicalRel]))

    // convert FlinkPhysicalRel DAG to ExecNodeGraph
    val generator = new ExecNodeGraphGenerator()
      //优化后的节点生成execGraph
    val execGraph =
      generator.generate(optimizedRelNodes.map(_.asInstanceOf[FlinkPhysicalRel]), isCompiled)

    // process the graph
    val context = new ProcessorContext(this)
    val processors = getExecNodeGraphProcessors
    processors.foldLeft(execGraph)((graph, processor) =&gt; processor.process(graph, context))
  }
</pre></table></code></div></div><p>最后ExecNodeGraph生成transformations<br />StreamPlanner为例</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre>//StreamPlanner
override protected def translateToPlan(execGraph: ExecNodeGraph): util.List[Transformation[_]] = {
    beforeTranslation()
    val planner = createDummyPlanner()
    val transformations = execGraph.getRootNodes.map {
        case node: StreamExecNode[_] =&gt; node.translateToPlan(planner)//ExecNodeBase
    case _ =&gt;
        throw new TableException(
            "Cannot generate DataStream due to an invalid logical plan. " +
            "This is a bug and should not happen. Please file an issue.")
    }
    afterTranslation()
    transformations
}
</pre></table></code></div></div><p>ExecNodeBase<RowData></RowData></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre>public final Transformation&lt;T&gt; translateToPlan(Planner planner) {
    if (transformation == null) {
        //translateToPlanInternal有子类实现，比如StreamExecJoin,StreamExecWindowJoin
        transformation =
        translateToPlanInternal(
            (PlannerBase) planner,
            new ExecNodeConfig(
                ((PlannerBase) planner).getTableConfig(), persistedConfig));
        if (this instanceof SingleTransformationTranslator) {
            if (inputsContainSingleton()) {
                transformation.setParallelism(1);
                transformation.setMaxParallelism(1);
            }
        }
    }
    return transformation;
}
</pre></table></code></div></div><p>translateToPlanInternal由具体子类实现，比如StreamExecJoin,StreamExecWindowJoin<br />FlinkBatchProgram<br />FlinkStreamProgram</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/flink1-15/'>Flink1.15</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/flink/" class="post-tag no-text-decoration" >flink</a> <a href="/tags/calcite/" class="post-tag no-text-decoration" >Calcite</a> <a href="/tags/realtime/" class="post-tag no-text-decoration" >realtime</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=FlinkSQL%28Flink1.15%2F1.16%29%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90+-+2pc&url=%2Fposts%2FFLINKSQL%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=FlinkSQL%28Flink1.15%2F1.16%29%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90+-+2pc&u=%2Fposts%2FFLINKSQL%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=%2Fposts%2FFLINKSQL%2F&text=FlinkSQL%28Flink1.15%2F1.16%29%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90+-+2pc" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/dubbo_threadpool/">Dubbo线程池问题汇总</a><li><a href="/posts/rocketMQ_max_maxReconsumeTimes/">RocketMQ 最大消费次数maxReconsumeTimes</a><li><a href="/posts/CF_forkjoin/">CompletableFuture与ForkJoinPool</a><li><a href="/posts/Elasticsearch-notes/">Elasticsearch笔记</a><li><a href="/posts/Solr5/">Solr5.x &&SolrCloud</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/realtime/">realtime</a> <a class="post-tag" href="/tags/flink/">flink</a> <a class="post-tag" href="/tags/distributed/">distributed</a> <a class="post-tag" href="/tags/java/">Java</a> <a class="post-tag" href="/tags/datascience/">Datascience</a> <a class="post-tag" href="/tags/ml/">ML</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/streamsets/">streamsets</a> <a class="post-tag" href="/tags/search/">Search</a> <a class="post-tag" href="/tags/file/">File</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/flink_svg/"><div class="card-body"> <em class="small" data-ts="1663545600" data-df="ll" > Sep 19, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Flink流程图整理以及画图模板</h3><div class="text-muted small"><p> [drawio项目文件地址]（https://github.com/2pc/mydrawio） Checkpoint流程 启动流程 FlinkSQL Flink Slot管理 画图模板 画图模板PNG</p></div></div></a></div><div class="card"> <a href="/posts/slot/"><div class="card-body"> <em class="small" data-ts="1660003200" data-df="ll" > Aug 9, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Flink1.15 SLOT源码分析</h3><div class="text-muted small"><p> Flink Slot管理 JobMaster继承了RpcEndpoint，start的回调Onstart protected void onStart() throws JobMasterException { try { startJobExecution(); } catch (Exception e) { final JobMaster...</p></div></div></a></div><div class="card"> <a href="/posts/master/"><div class="card-body"> <em class="small" data-ts="1660608000" data-df="ll" > Aug 16, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Flink1.15 JobMaster源码分析</h3><div class="text-muted small"><p> 启动流程 dispatcher.start();调用start后回调onStart public final void start() { rpcServer.start(); } StandaloneDispatcher继承自Dispatcher public class StandaloneDispatcher extends Dispatcher ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/calcite_rule/" class="btn btn-outline-primary" prompt="Older"><p>FlinkSQL(Flink1.15)规则优化以及Calcite原理</p></a> <a href="/posts/vscode_server/" class="btn btn-outline-primary" prompt="Newer"><p>云服务器部署code-server无域名支持https</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/2pc">2pc</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/realtime/">realtime</a> <a class="post-tag" href="/tags/flink/">flink</a> <a class="post-tag" href="/tags/distributed/">distributed</a> <a class="post-tag" href="/tags/java/">Java</a> <a class="post-tag" href="/tags/datascience/">Datascience</a> <a class="post-tag" href="/tags/ml/">ML</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/streamsets/">streamsets</a> <a class="post-tag" href="/tags/search/">Search</a> <a class="post-tag" href="/tags/file/">File</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
